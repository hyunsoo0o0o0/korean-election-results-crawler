# Korean Election Results Crawler

A comprehensive Python toolkit for downloading, parsing, and analyzing election results from the Korean National Election Commission (ì¤‘ì•™ì„ ê±°ê´€ë¦¬ìœ„ì›íšŒ).

## Note by a human

This project is mostly LLM-generated, with the only exception of this section right here.
The project is released with no strings attached. (CC0, to be exact)
Cline + Claude Sonnet 4 is used for the vibe coding session.

## ğŸ—³ï¸ Overview

This project provides tools to systematically download election result files from the Korean National Election Commission's website, convert them to CSV format, and merge them into consolidated datasets for analysis. It supports various Korean election types including presidential, national assembly, and local elections.

## âœ¨ Features

### Core Crawler (`election_crawler.py`)
- **Robust Error Handling**: Automatic retry with exponential backoff
- **Rate Limiting**: Configurable delays with jitter to avoid overwhelming servers
- **Progress Tracking**: Comprehensive logging and statistics
- **File Validation**: Automatic deduplication and size checking
- **Concurrent Downloads**: Optional multi-threaded downloading
- **Korean Support**: Proper handling of Korean filenames and encoding
- **Command Line Interface**: Full CLI with configurable options

### Data Processing Pipeline
- **HTML to CSV Conversion**: Parse election result tables from HTML/XLS files
- **Data Merging**: Combine multiple CSV files with location mapping
- **Column Standardization**: Reorder columns to consistent format
- **Data Cleaning**: Handle comma-separated numbers and missing values

### Additional Tools
- **Test Suite**: Validate functionality before full crawls
- **Migration Guide**: Upgrade path from legacy implementations
- **Comprehensive Logging**: File and console logging with multiple levels

## ğŸ“¦ Installation

### Prerequisites
- Python 3.7 or higher
- Internet connection for downloading election data

### Setup
1. Clone or download the project:
   ```bash
   git clone <repository-url>
   cd korean-election-results-crawler
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Test the installation:
   ```bash
   python test_crawler.py
   ```

## ğŸš€ Quick Start

### Basic Usage
Download all election results for the default election:
```bash
python election_crawler.py
```

### With Options
```bash
# Use concurrent downloads (faster)
python election_crawler.py --concurrent --max-workers 3

# Different election
python election_crawler.py --election-id 0020220309 --election-code 2

# Custom output directory
python election_crawler.py --download-dir my_election_data

# Enable debug logging
python election_crawler.py --log-level DEBUG
```

### Processing Pipeline
```bash
# 1. Download election results
python election_crawler.py

# 2. Convert HTML/XLS files to CSV
python html_to_csv_parser.py

# 3. Merge all CSV files
python csv_merger.py

# 4. Reorder columns (optional)
python csv_column_reorder.py merged_election_results.csv
```

## ğŸ“ Project Structure

```
korean-election-results-crawler/
â”œâ”€â”€ election_crawler.py      # Main crawler (enhanced)
â”œâ”€â”€ config.py               # Configuration settings
â”œâ”€â”€ html_to_csv_parser.py   # HTML/XLS to CSV converter
â”œâ”€â”€ csv_merger.py           # CSV file merger
â”œâ”€â”€ csv_column_reorder.py   # Column reordering utility
â”œâ”€â”€ test_crawler.py         # Test suite
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ .gitignore             # Git ignore rules
â””â”€â”€ reordered_merged_results.csv # Final processed data
```

## âš™ï¸ Configuration

### Election Settings (`config.py`)
- `DEFAULT_ELECTION_ID`: Election identifier (e.g., "0020250603")
- `DEFAULT_ELECTION_CODE`: Election type code (1=Presidential, 2=National Assembly, etc.)
- `DOWNLOAD_DIR`: Directory for downloaded files

### Performance Settings
- `MAX_WORKERS`: Concurrent download threads (default: 3)
- `BASE_DELAY`: Delay between requests (default: 1 second)
- `CITY_DELAY`: Delay between cities (default: 2 seconds)
- `MAX_RETRIES`: Maximum retry attempts (default: 3)

### Logging Configuration
- `LOG_LEVEL`: Logging verbosity (DEBUG, INFO, WARNING, ERROR)
- `LOG_FILE`: Log file location (default: 'election_crawler.log')

## ğŸ“Š Supported Election Types

| Code | Type | Korean Name |
|------|------|-------------|
| 1 | Presidential | ëŒ€í†µë ¹ì„ ê±° |
| 2 | National Assembly | êµ­íšŒì˜ì›ì„ ê±° |
| 3 | Local - Governor | ì§€ë°©ì„ ê±° - ì‹œë„ì§€ì‚¬ |
| 4 | Local - Mayor | ì§€ë°©ì„ ê±° - ì‹œì¥êµ°ìˆ˜êµ¬ì²­ì¥ |
| 5 | Metropolitan Council | ê´‘ì—­ì˜íšŒì˜ì› |
| 6 | Basic Council | ê¸°ì´ˆì˜íšŒì˜ì› |
| 7 | Overseas Voting | ì¬ì™¸ì„ ê±° |
| 8 | By-elections | ë³´ê¶ì„ ê±° |

## ğŸ”§ Advanced Usage

### Programmatic Usage
```python
from election_crawler import ElectionCrawler

# Create crawler instance
crawler = ElectionCrawler(
    election_id="0020250603",
    election_code="1",
    download_dir="my_results"
)

# Sequential crawling
crawler.crawl_all_locations()

# Concurrent crawling
crawler.crawl_all_locations(use_concurrent=True, max_workers=5)

# Access statistics
print(f"Downloaded: {crawler.stats['downloaded']}")
print(f"Errors: {crawler.stats['errors']}")
print(f"Total size: {crawler.stats['total_size']} bytes")
```

### Custom Data Processing
```python
from html_to_csv_parser import ElectionHTMLParser
from csv_merger import CSVMerger

# Parse HTML files to CSV
parser = ElectionHTMLParser("election_results", "csv_results")
stats = parser.parse_all_files()

# Merge CSV files
merger = CSVMerger("csv_results", "merged_results.csv")
merger.merge_all()
```

### Column Reordering
```python
from csv_column_reorder import reorder_csv_columns

# Reorder single file
reorder_csv_columns("merged_results.csv", "reordered_results.csv")

# Reorder multiple files
from csv_column_reorder import reorder_multiple_csv_files
reorder_multiple_csv_files("csv_results", "csv_results_reordered")
```

## ğŸ“‹ Command Line Reference

### election_crawler.py
```bash
python election_crawler.py [OPTIONS]

Options:
  --election-id TEXT      Election ID to crawl (default: 0020250603)
  --election-code TEXT    Election type code (default: 1)
  --download-dir TEXT     Directory to save files (default: election_results)
  --concurrent           Use concurrent downloads
  --max-workers INTEGER  Maximum concurrent workers (default: 3)
  --log-level TEXT       Logging level (DEBUG, INFO, WARNING, ERROR)
  --help                 Show help message
```

### html_to_csv_parser.py
```bash
python html_to_csv_parser.py [OPTIONS]

Options:
  --input-dir TEXT       Directory containing HTML files (default: election_results)
  --output-dir TEXT      Directory to save CSV files (default: csv_results)
  --create-summary       Create summary CSV file
  --help                 Show help message
```

### csv_column_reorder.py
```bash
python csv_column_reorder.py [OPTIONS] INPUT

Arguments:
  INPUT                  Input CSV file or directory

Options:
  -o, --output TEXT      Output file or directory
  -d, --directory        Process all CSV files in directory
  -p, --pattern TEXT     File pattern for directory processing (default: *.csv)
  --help                 Show help message
```

## ğŸ› ï¸ Data Schema

### Raw Election Data Columns
- `ì‹œë„ëª…`: City/Province name
- `ì‹œêµ°êµ¬ëª…`: District name  
- `ìë©´ë™ëª…`: Town/Village name
- `íˆ¬í‘œêµ¬ëª…`: Voting district name
- `ì„ ê±°ì¸ìˆ˜`: Number of eligible voters
- `íˆ¬í‘œìˆ˜`: Number of votes cast
- `[í›„ë³´ìëª…]`: Vote counts for each candidate
- `ê³„`: Total valid votes
- `ë¬´íš¨íˆ¬í‘œìˆ˜`: Invalid votes
- `ê¸°ê¶Œììˆ˜`: Abstentions

### Processed Data Features
- Consistent column ordering
- Numeric data type conversion
- Location name standardization
- Data validation and cleaning

## ğŸ” Troubleshooting

### Common Issues

**Connection Errors**
```
ElectionCrawlerError: Failed to make request after 3 attempts
```
- Check internet connection
- Verify the election commission website is accessible
- Try reducing `MAX_WORKERS` for concurrent downloads

**Permission Errors**
```
PermissionError: [Errno 13] Permission denied
```
- Ensure write permissions to download directory
- Run with appropriate user privileges
- Check disk space availability

**Encoding Issues**
```
UnicodeDecodeError: 'utf-8' codec can't decode
```
- Files are automatically handled with proper Korean encoding
- Check if input files are corrupted
- Try re-downloading problematic files

**Memory Issues with Large Datasets**
- Process files in smaller batches
- Use the `--max-workers 1` option for sequential processing
- Increase system memory or use disk-based processing

### Debug Mode
Enable detailed logging:
```bash
python election_crawler.py --log-level DEBUG
```

Check log file:
```bash
tail -f election_crawler.log
```

### Testing
Validate functionality before full crawl:
```bash
python test_crawler.py
```

## ğŸ“ Logging

The crawler provides comprehensive logging:

- **Console Output**: Progress and summary information
- **File Logging**: Detailed operation logs in `election_crawler.log`
- **Statistics**: Download counts, error rates, processing times

Log levels:
- `DEBUG`: Detailed debugging information
- `INFO`: General progress information (default)
- `WARNING`: Non-critical issues
- `ERROR`: Critical errors

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## âš–ï¸ Legal Notice

This tool is for educational and research purposes. Users are responsible for:
- Complying with the Korean National Election Commission's terms of service
- Respecting rate limits and server resources
- Using downloaded data in accordance with applicable laws

## ğŸ“„ License

This project is provided as-is for educational purposes. Please respect the terms of service of the Korean National Election Commission when using this tool.

## ğŸ”— Related Resources

- [Korean National Election Commission](http://info.nec.go.kr)
- [Election Data Portal](http://info.nec.go.kr/main/main_load.xhtml)
- [Python Requests Documentation](https://docs.python-requests.org/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
